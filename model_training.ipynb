{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned_steam_data.csv\", encoding='utf8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpful</th>\n",
       "      <th>hour_played</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>...</th>\n",
       "      <th>encoded_1335</th>\n",
       "      <th>encoded_1336</th>\n",
       "      <th>encoded_1337</th>\n",
       "      <th>encoded_1338</th>\n",
       "      <th>encoded_1339</th>\n",
       "      <th>encoded_1340</th>\n",
       "      <th>encoded_1341</th>\n",
       "      <th>encoded_1342</th>\n",
       "      <th>encoded_1343</th>\n",
       "      <th>encoded_1344</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>578</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>523</td>\n",
       "      <td>170</td>\n",
       "      <td>1193</td>\n",
       "      <td>523</td>\n",
       "      <td>264</td>\n",
       "      <td>608</td>\n",
       "      <td>307</td>\n",
       "      <td>1143</td>\n",
       "      <td>794</td>\n",
       "      <td>1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>892</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1106</td>\n",
       "      <td>44</td>\n",
       "      <td>921</td>\n",
       "      <td>633</td>\n",
       "      <td>776</td>\n",
       "      <td>902</td>\n",
       "      <td>794</td>\n",
       "      <td>239</td>\n",
       "      <td>824</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1086</td>\n",
       "      <td>676</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>878</td>\n",
       "      <td>178</td>\n",
       "      <td>394</td>\n",
       "      <td>1073</td>\n",
       "      <td>1258</td>\n",
       "      <td>1395</td>\n",
       "      <td>923</td>\n",
       "      <td>932</td>\n",
       "      <td>1015</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2139</td>\n",
       "      <td>612</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>178</td>\n",
       "      <td>259</td>\n",
       "      <td>974</td>\n",
       "      <td>1143</td>\n",
       "      <td>832</td>\n",
       "      <td>1464</td>\n",
       "      <td>956</td>\n",
       "      <td>65</td>\n",
       "      <td>619</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpful  hour_played  Year  Month  Day  encoded_1  encoded_2  encoded_3  \\\n",
       "0        4          578  2019      2   10          0          0          0   \n",
       "1        0          184  2019      2   10          0          0          0   \n",
       "2        0          892  2019      2    7          0          0          0   \n",
       "3     1086          676  2018      6   14          0          0          0   \n",
       "4     2139          612  2017      6   20          0          0          0   \n",
       "\n",
       "   encoded_4  encoded_5  ...  encoded_1335  encoded_1336  encoded_1337  \\\n",
       "0          0          0  ...           523           170          1193   \n",
       "1          0          0  ...             0             0             0   \n",
       "2          0          0  ...          1106            44           921   \n",
       "3          0          0  ...           878           178           394   \n",
       "4          0          0  ...           178           259           974   \n",
       "\n",
       "   encoded_1338  encoded_1339  encoded_1340  encoded_1341  encoded_1342  \\\n",
       "0           523           264           608           307          1143   \n",
       "1             0             0             0             0             0   \n",
       "2           633           776           902           794           239   \n",
       "3          1073          1258          1395           923           932   \n",
       "4          1143           832          1464           956            65   \n",
       "\n",
       "   encoded_1343  encoded_1344  \n",
       "0           794          1336  \n",
       "1             0           811  \n",
       "2           824           997  \n",
       "3          1015           285  \n",
       "4           619            91  \n",
       "\n",
       "[5 rows x 1349 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols = ['funny', 'is_early_access_review', 'recommendation', 'review',\n",
    "            'title', 'cleaned_reviews']\n",
    "df.drop(drop_cols, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = [x for x in df.columns.tolist() if x.startswith(\"encoded\")]\n",
    "len(text_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>encoded_6</th>\n",
       "      <th>encoded_7</th>\n",
       "      <th>encoded_8</th>\n",
       "      <th>encoded_9</th>\n",
       "      <th>encoded_10</th>\n",
       "      <th>...</th>\n",
       "      <th>encoded_1335</th>\n",
       "      <th>encoded_1336</th>\n",
       "      <th>encoded_1337</th>\n",
       "      <th>encoded_1338</th>\n",
       "      <th>encoded_1339</th>\n",
       "      <th>encoded_1340</th>\n",
       "      <th>encoded_1341</th>\n",
       "      <th>encoded_1342</th>\n",
       "      <th>encoded_1343</th>\n",
       "      <th>encoded_1344</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>523</td>\n",
       "      <td>170</td>\n",
       "      <td>1193</td>\n",
       "      <td>523</td>\n",
       "      <td>264</td>\n",
       "      <td>608</td>\n",
       "      <td>307</td>\n",
       "      <td>1143</td>\n",
       "      <td>794</td>\n",
       "      <td>1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1106</td>\n",
       "      <td>44</td>\n",
       "      <td>921</td>\n",
       "      <td>633</td>\n",
       "      <td>776</td>\n",
       "      <td>902</td>\n",
       "      <td>794</td>\n",
       "      <td>239</td>\n",
       "      <td>824</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>878</td>\n",
       "      <td>178</td>\n",
       "      <td>394</td>\n",
       "      <td>1073</td>\n",
       "      <td>1258</td>\n",
       "      <td>1395</td>\n",
       "      <td>923</td>\n",
       "      <td>932</td>\n",
       "      <td>1015</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>178</td>\n",
       "      <td>259</td>\n",
       "      <td>974</td>\n",
       "      <td>1143</td>\n",
       "      <td>832</td>\n",
       "      <td>1464</td>\n",
       "      <td>956</td>\n",
       "      <td>65</td>\n",
       "      <td>619</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   encoded_1  encoded_2  encoded_3  encoded_4  encoded_5  encoded_6  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   encoded_7  encoded_8  encoded_9  encoded_10  ...  encoded_1335  \\\n",
       "0          0          0          0           0  ...           523   \n",
       "1          0          0          0           0  ...             0   \n",
       "2          0          0          0           0  ...          1106   \n",
       "3          0          0          0           0  ...           878   \n",
       "4          0          0          0           0  ...           178   \n",
       "\n",
       "   encoded_1336  encoded_1337  encoded_1338  encoded_1339  encoded_1340  \\\n",
       "0           170          1193           523           264           608   \n",
       "1             0             0             0             0             0   \n",
       "2            44           921           633           776           902   \n",
       "3           178           394          1073          1258          1395   \n",
       "4           259           974          1143           832          1464   \n",
       "\n",
       "   encoded_1341  encoded_1342  encoded_1343  encoded_1344  \n",
       "0           307          1143           794          1336  \n",
       "1             0             0             0           811  \n",
       "2           794           239           824           997  \n",
       "3           923           932          1015           285  \n",
       "4           956            65           619            91  \n",
       "\n",
       "[5 rows x 1344 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = df[text_columns]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pytorch model\n",
    "DROPOUT = 0.1\n",
    "\n",
    "class Attention_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention_Net, self).__init__()\n",
    "        \n",
    "        # define architecture\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "#         self.embedding_dropout = nn.Dropout2d(DROPOUT) # take this out potentially\n",
    "        \n",
    "        self.lstm = nn.LSTM(EMBED_DIM, LSTM_DIM, bidirectional=True, batch_first=True) #batch_first=True?\n",
    "        \n",
    "        # attention layer\n",
    "        self.attention_layer = Attention(LSTM_DIM * 2, MAX_SEQ_LEN) # param here could be wrong\n",
    "        \n",
    "        self.out = nn.Linear(LSTM_DIM * 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0)).view(1, MAX_SEQ_LEN, -1)\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_lstm_atten = self.attention_layer(h_lstm)\n",
    "        out = self.out(h_lstm_atten)\n",
    "        return out\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1344])\n",
      "torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "train_X = torch.tensor(text_df.values)\n",
    "train_Y = torch.tensor([[x] for x in df.helpful.values]).float()\n",
    "\n",
    "print(train_X.size())\n",
    "print(train_Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = len(text_columns)\n",
    "VOCAB_SIZE = 1492 # should ideally just transport this from prev\n",
    "EMBED_DIM = 64\n",
    "LSTM_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = Attention_Net()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(attention_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# out = attention_model(train_X)\n",
    "# out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_y = tensor([[0.1319]], grad_fn=<AddmmBackward>), actual = tensor([4.])\n",
      "loss =  tensor(14.9626, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.2321]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.0539, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.2194]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.0481, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.1167]], grad_fn=<AddmmBackward>), actual = tensor([1086.])\n",
      "loss =  tensor(1179142.5000, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[11.3361]], grad_fn=<AddmmBackward>), actual = tensor([2139.])\n",
      "loss =  tensor(4526953.5000, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[121.6173]], grad_fn=<AddmmBackward>), actual = tensor([55.])\n",
      "loss =  tensor(4437.8647, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[127.6925]], grad_fn=<AddmmBackward>), actual = tensor([228.])\n",
      "loss =  tensor(10061.5908, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[170.8452]], grad_fn=<AddmmBackward>), actual = tensor([219.])\n",
      "loss =  tensor(2318.8877, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[171.5171]], grad_fn=<AddmmBackward>), actual = tensor([54.])\n",
      "loss =  tensor(13810.2773, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[163.0951]], grad_fn=<AddmmBackward>), actual = tensor([271.])\n",
      "loss =  tensor(11643.4629, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[162.4153]], grad_fn=<AddmmBackward>), actual = tensor([106.])\n",
      "loss =  tensor(3182.6807, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[151.3191]], grad_fn=<AddmmBackward>), actual = tensor([614.])\n",
      "loss =  tensor(214073.6250, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[232.5257]], grad_fn=<AddmmBackward>), actual = tensor([123.])\n",
      "loss =  tensor(11995.8779, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[213.9158]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(45759.9688, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[184.0520]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(33875.1445, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[155.3050]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(24119.6309, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[132.8677]], grad_fn=<AddmmBackward>), actual = tensor([1.])\n",
      "loss =  tensor(17389.0879, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[113.6314]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(12912.0938, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[95.2568]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(9073.8652, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[82.7150]], grad_fn=<AddmmBackward>), actual = tensor([1.])\n",
      "loss =  tensor(6677.3418, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[70.4643]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(4965.2202, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[59.8029]], grad_fn=<AddmmBackward>), actual = tensor([1.])\n",
      "loss =  tensor(3457.7861, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[50.3720]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(2537.3406, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[43.7043]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(1910.0638, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[37.3750]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(1396.8909, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[31.7561]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(1008.4517, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[25.2982]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(639.9969, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[23.1649]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(536.6115, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[19.4947]], grad_fn=<AddmmBackward>), actual = tensor([1.])\n",
      "loss =  tensor(342.0538, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[9.4206]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(88.7479, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[16.1207]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(259.8771, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[13.6181]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(185.4514, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[11.4925]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(132.0786, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[9.7506]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(95.0736, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[8.0540]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(64.8676, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[6.9167]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(47.8407, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[6.0556]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(36.6700, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[5.1101]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(26.1127, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[4.3394]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(18.8304, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[3.6870]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(13.5937, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[3.2183]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(10.3576, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[2.6724]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(7.1418, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[2.2764]], grad_fn=<AddmmBackward>), actual = tensor([1.])\n",
      "loss =  tensor(1.6292, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[2.0792]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(4.3231, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[1.8092]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(3.2733, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[1.4879]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(2.2139, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[1.2970]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(1.6822, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[1.0489]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(1.1002, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.9021]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.8138, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.7611]], grad_fn=<AddmmBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.5793, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "EPOCHS = 1\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    for idx, train_x_example in enumerate(train_X):\n",
    "        attention_model.zero_grad()\n",
    "        pred_y = attention_model(train_x_example)\n",
    "        print (\"pred_y = {}, actual = {}\".format(pred_y, train_Y[idx]))\n",
    "        loss = loss_function(pred_y, train_Y[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print (\"loss = \", loss)\n",
    "        print (\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted helpful =  tensor([[0.6429]])\n",
      "actual helpful =  tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "with torch.no_grad():\n",
    "    \n",
    "    preds = attention_model(train_X[0])\n",
    "    print ('predicted helpful = ', preds)\n",
    "    print ('actual helpful = ', train_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
