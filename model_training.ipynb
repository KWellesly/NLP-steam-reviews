{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned_steam_data.csv\", encoding='utf8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpful</th>\n",
       "      <th>hour_played</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>...</th>\n",
       "      <th>encoded_1335</th>\n",
       "      <th>encoded_1336</th>\n",
       "      <th>encoded_1337</th>\n",
       "      <th>encoded_1338</th>\n",
       "      <th>encoded_1339</th>\n",
       "      <th>encoded_1340</th>\n",
       "      <th>encoded_1341</th>\n",
       "      <th>encoded_1342</th>\n",
       "      <th>encoded_1343</th>\n",
       "      <th>encoded_1344</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>578</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>523</td>\n",
       "      <td>170</td>\n",
       "      <td>1193</td>\n",
       "      <td>523</td>\n",
       "      <td>264</td>\n",
       "      <td>608</td>\n",
       "      <td>307</td>\n",
       "      <td>1143</td>\n",
       "      <td>794</td>\n",
       "      <td>1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>892</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1106</td>\n",
       "      <td>44</td>\n",
       "      <td>921</td>\n",
       "      <td>633</td>\n",
       "      <td>776</td>\n",
       "      <td>902</td>\n",
       "      <td>794</td>\n",
       "      <td>239</td>\n",
       "      <td>824</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1086</td>\n",
       "      <td>676</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>878</td>\n",
       "      <td>178</td>\n",
       "      <td>394</td>\n",
       "      <td>1073</td>\n",
       "      <td>1258</td>\n",
       "      <td>1395</td>\n",
       "      <td>923</td>\n",
       "      <td>932</td>\n",
       "      <td>1015</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2139</td>\n",
       "      <td>612</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>178</td>\n",
       "      <td>259</td>\n",
       "      <td>974</td>\n",
       "      <td>1143</td>\n",
       "      <td>832</td>\n",
       "      <td>1464</td>\n",
       "      <td>956</td>\n",
       "      <td>65</td>\n",
       "      <td>619</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpful  hour_played  Year  Month  Day  encoded_1  encoded_2  encoded_3  \\\n",
       "0        4          578  2019      2   10          0          0          0   \n",
       "1        0          184  2019      2   10          0          0          0   \n",
       "2        0          892  2019      2    7          0          0          0   \n",
       "3     1086          676  2018      6   14          0          0          0   \n",
       "4     2139          612  2017      6   20          0          0          0   \n",
       "\n",
       "   encoded_4  encoded_5  ...  encoded_1335  encoded_1336  encoded_1337  \\\n",
       "0          0          0  ...           523           170          1193   \n",
       "1          0          0  ...             0             0             0   \n",
       "2          0          0  ...          1106            44           921   \n",
       "3          0          0  ...           878           178           394   \n",
       "4          0          0  ...           178           259           974   \n",
       "\n",
       "   encoded_1338  encoded_1339  encoded_1340  encoded_1341  encoded_1342  \\\n",
       "0           523           264           608           307          1143   \n",
       "1             0             0             0             0             0   \n",
       "2           633           776           902           794           239   \n",
       "3          1073          1258          1395           923           932   \n",
       "4          1143           832          1464           956            65   \n",
       "\n",
       "   encoded_1343  encoded_1344  \n",
       "0           794          1336  \n",
       "1             0           811  \n",
       "2           824           997  \n",
       "3          1015           285  \n",
       "4           619            91  \n",
       "\n",
       "[5 rows x 1349 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols = ['funny', 'is_early_access_review', 'recommendation', 'review',\n",
    "            'title', 'cleaned_reviews']\n",
    "df.drop(drop_cols, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = [x for x in df.columns.tolist() if x.startswith(\"encoded\")]\n",
    "len(text_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>encoded_6</th>\n",
       "      <th>encoded_7</th>\n",
       "      <th>encoded_8</th>\n",
       "      <th>encoded_9</th>\n",
       "      <th>encoded_10</th>\n",
       "      <th>...</th>\n",
       "      <th>encoded_1335</th>\n",
       "      <th>encoded_1336</th>\n",
       "      <th>encoded_1337</th>\n",
       "      <th>encoded_1338</th>\n",
       "      <th>encoded_1339</th>\n",
       "      <th>encoded_1340</th>\n",
       "      <th>encoded_1341</th>\n",
       "      <th>encoded_1342</th>\n",
       "      <th>encoded_1343</th>\n",
       "      <th>encoded_1344</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>523</td>\n",
       "      <td>170</td>\n",
       "      <td>1193</td>\n",
       "      <td>523</td>\n",
       "      <td>264</td>\n",
       "      <td>608</td>\n",
       "      <td>307</td>\n",
       "      <td>1143</td>\n",
       "      <td>794</td>\n",
       "      <td>1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1106</td>\n",
       "      <td>44</td>\n",
       "      <td>921</td>\n",
       "      <td>633</td>\n",
       "      <td>776</td>\n",
       "      <td>902</td>\n",
       "      <td>794</td>\n",
       "      <td>239</td>\n",
       "      <td>824</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>878</td>\n",
       "      <td>178</td>\n",
       "      <td>394</td>\n",
       "      <td>1073</td>\n",
       "      <td>1258</td>\n",
       "      <td>1395</td>\n",
       "      <td>923</td>\n",
       "      <td>932</td>\n",
       "      <td>1015</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>178</td>\n",
       "      <td>259</td>\n",
       "      <td>974</td>\n",
       "      <td>1143</td>\n",
       "      <td>832</td>\n",
       "      <td>1464</td>\n",
       "      <td>956</td>\n",
       "      <td>65</td>\n",
       "      <td>619</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   encoded_1  encoded_2  encoded_3  encoded_4  encoded_5  encoded_6  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   encoded_7  encoded_8  encoded_9  encoded_10  ...  encoded_1335  \\\n",
       "0          0          0          0           0  ...           523   \n",
       "1          0          0          0           0  ...             0   \n",
       "2          0          0          0           0  ...          1106   \n",
       "3          0          0          0           0  ...           878   \n",
       "4          0          0          0           0  ...           178   \n",
       "\n",
       "   encoded_1336  encoded_1337  encoded_1338  encoded_1339  encoded_1340  \\\n",
       "0           170          1193           523           264           608   \n",
       "1             0             0             0             0             0   \n",
       "2            44           921           633           776           902   \n",
       "3           178           394          1073          1258          1395   \n",
       "4           259           974          1143           832          1464   \n",
       "\n",
       "   encoded_1341  encoded_1342  encoded_1343  encoded_1344  \n",
       "0           307          1143           794          1336  \n",
       "1             0             0             0           811  \n",
       "2           794           239           824           997  \n",
       "3           923           932          1015           285  \n",
       "4           956            65           619            91  \n",
       "\n",
       "[5 rows x 1344 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = df[text_columns]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pytorch model\n",
    "DROPOUT = 0.1\n",
    "\n",
    "class Attention_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention_Net, self).__init__()\n",
    "        \n",
    "        # define architecture\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "#         self.embedding_dropout = nn.Dropout2d(DROPOUT) # take this out potentially\n",
    "        \n",
    "        self.lstm = nn.LSTM(EMBED_DIM, LSTM_DIM, bidirectional=True, batch_first=True) # batch_first=True?\n",
    "        \n",
    "        # attention layer\n",
    "        self.attention_layer = Attention(LSTM_DIM * 2, MAX_SEQ_LEN) # param here could be wrong\n",
    "        \n",
    "        self.linear = nn.Linear(LSTM_DIM * 2, 1)\n",
    "        \n",
    "        self.out = nn.Softplus() # test addition of this\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0)).view(1, MAX_SEQ_LEN, -1)\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_lstm_atten = self.attention_layer(h_lstm)\n",
    "        out = self.linear(h_lstm_atten)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1344])\n",
      "torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "train_X = torch.tensor(text_df.values)\n",
    "train_Y = torch.tensor([[x] for x in df.helpful.values]).float()\n",
    "\n",
    "print(train_X.size())\n",
    "print(train_Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = len(text_columns)\n",
    "VOCAB_SIZE = 1492 # should ideally just transport this from prev\n",
    "EMBED_DIM = 64\n",
    "LSTM_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = Attention_Net()\n",
    "# loss_function = nn.MSELoss()\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "optimizer = optim.SGD(attention_model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# out = attention_model(train_X)\n",
    "# out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_y = tensor([[0.7078]], grad_fn=<SoftplusBackward>), actual = tensor([4.])\n",
      "loss =  tensor(2.7922, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.7480]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.2797, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.7163]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.2565, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.6888]], grad_fn=<SoftplusBackward>), actual = tensor([1086.])\n",
      "loss =  tensor(1084.8112, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.7210]], grad_fn=<SoftplusBackward>), actual = tensor([2139.])\n",
      "loss =  tensor(2137.7791, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.7433]], grad_fn=<SoftplusBackward>), actual = tensor([55.])\n",
      "loss =  tensor(53.7567, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.7803]], grad_fn=<SoftplusBackward>), actual = tensor([228.])\n",
      "loss =  tensor(226.7197, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.8321]], grad_fn=<SoftplusBackward>), actual = tensor([219.])\n",
      "loss =  tensor(217.6679, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.8653]], grad_fn=<SoftplusBackward>), actual = tensor([54.])\n",
      "loss =  tensor(52.6347, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.9305]], grad_fn=<SoftplusBackward>), actual = tensor([271.])\n",
      "loss =  tensor(269.5695, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.9678]], grad_fn=<SoftplusBackward>), actual = tensor([106.])\n",
      "loss =  tensor(104.5322, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[1.0107]], grad_fn=<SoftplusBackward>), actual = tensor([614.])\n",
      "loss =  tensor(612.4893, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[1.1030]], grad_fn=<SoftplusBackward>), actual = tensor([123.])\n",
      "loss =  tensor(121.3969, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[1.1695]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.6695, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[1.1013]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.6013, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[1.0340]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.5340, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.9728]], grad_fn=<SoftplusBackward>), actual = tensor([1.])\n",
      "loss =  tensor(0.0004, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.9746]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.4749, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.9148]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.4184, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.8692]], grad_fn=<SoftplusBackward>), actual = tensor([1.])\n",
      "loss =  tensor(0.0086, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.8757]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.3835, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.8308]], grad_fn=<SoftplusBackward>), actual = tensor([1.])\n",
      "loss =  tensor(0.0143, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.8370]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.3503, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.8001]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.3201, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.7644]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.2922, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.7323]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.2681, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.7028]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.2469, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.6783]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.2300, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.6541]], grad_fn=<SoftplusBackward>), actual = tensor([1.])\n",
      "loss =  tensor(0.0598, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.6938]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.2407, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.6642]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.2206, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.6412]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.2056, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.6204]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1925, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.6014]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1808, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.5866]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1721, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.5728]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1641, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.5517]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1522, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.5382]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1448, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.5255]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1381, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.5110]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1305, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.5207]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1356, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.4885]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1193, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.4794]], grad_fn=<SoftplusBackward>), actual = tensor([1.])\n",
      "loss =  tensor(0.1355, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.4918]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1209, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.4816]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1160, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.4684]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1097, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.4608]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1062, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.4523]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.1023, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.4413]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.0974, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n",
      "pred_y = tensor([[0.4335]], grad_fn=<SoftplusBackward>), actual = tensor([0.])\n",
      "loss =  tensor(0.0940, grad_fn=<SmoothL1LossBackward>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "EPOCHS = 1\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    for idx, train_x_example in enumerate(train_X):\n",
    "        attention_model.zero_grad()\n",
    "        pred_y = attention_model(train_x_example)\n",
    "        print (\"pred_y = {}, actual = {}\".format(pred_y, train_Y[idx]))\n",
    "        loss = loss_function(pred_y, train_Y[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print (\"loss = \", loss)\n",
    "        print (\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted helpful =  tensor([[0.6429]])\n",
      "actual helpful =  tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "with torch.no_grad():\n",
    "    \n",
    "    preds = attention_model(train_X[0])\n",
    "    print ('predicted helpful = ', preds)\n",
    "    print ('actual helpful = ', train_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
