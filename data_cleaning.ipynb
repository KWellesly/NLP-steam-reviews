{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import re, itertools\n",
    "import string\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "import time\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# keras imports\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# text cleanup\n",
    "# from pattern.en import suggest\n",
    "import enchant\n",
    "from enchant.checker import SpellChecker\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/steam_reviews.csv\"\n",
    "df = pd.read_csv(data_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentile graphing below -- it takes a very long time to run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perc = np.percentile(df['helpful'], 99) #10,25,50,75,90\n",
    "d = df['helpful']\n",
    "print(np.max(df['helpful']))\n",
    "p = np.linspace(90, 100, 10)\n",
    "# p = [10.0,25.0,50.0,75.0,90.0]\n",
    "perc = np.percentile(d, p)\n",
    "\n",
    "plt.plot(d)\n",
    "plt.ylabel('# of Helpful Votes')\n",
    "plt.xlabel('Reviews')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(p, perc)\n",
    "plt.ylabel('# of Votes')\n",
    "plt.xlabel('Percentile')\n",
    "\n",
    "rec = df['recommendation']\n",
    "yes = df['recommendation'].sum()\n",
    "no = len(df['recommendation']) - yes\n",
    "print(yes)\n",
    "print(no)\n",
    "\n",
    "x = (\"Recommended\", \"Not Recommended\")\n",
    "y = [yes, no]\n",
    "fig2 = plt.figure()\n",
    "\n",
    "plt.bar(x, y, align='center', alpha=0.5)\n",
    "plt.ylabel('# of Reviews')\n",
    "plt.show()\n",
    "\n",
    "fig3 = plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove nan reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = len(df)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "print ('dropped {} nan reviews'.format(orig_len - len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "len(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moved the below code after removing nan reviews, also refactored for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df[\"review\"].tolist()\n",
    "r1 = []\n",
    "for i in range(len(r)):\n",
    "    word_len = len(r[i].split(\" \"))\n",
    "    r1.append(word_len)\n",
    "print (np.mean(r1))\n",
    "print (np.sum(r1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-hot encode title & early access & recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_early_access_review'] = df['is_early_access_review'].astype('int')\n",
    "df['recommendation'] = df['recommendation'].map({'Recommended':1, 'Not Recommended':0})\n",
    "df = pd.get_dummies(df, columns=['title'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only keep reviews <= 300 words\n",
    "* Need to calculate 10th, 25th, 50th, 75th, 90th percentile of helpful/funny votes on the removed reviews to unders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove reviews over a limit\n",
    "removed_idx = []\n",
    "for i in range(len(df)):\n",
    "    review = df.iloc[i].review\n",
    "    words = review.split(\" \")\n",
    "    if len(words) > 200: \n",
    "        removed_idx.append(i)\n",
    "\n",
    "\n",
    "df.drop(df.index[removed_idx], inplace=True)\n",
    "print (\"removed {} reviews > 200 words\".format(len(removed_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up textual data (try this stuff later, do vanilla run first)\n",
    "* Remove common stopwords?\n",
    "* Lower case everything \n",
    "* All ‘s in the data is preceded by a ‘\\’ which should be cleaned out\n",
    "* Maybe don't lower case everything... ex: \n",
    "    * \"I HIGHLY RECOMMEND THIS GAME CAUSE THERE ARE SOME KILLERS WILL FOLLOW YOU NO MATTER WHERE YOU GO IN WHICH YOU'LL GO LOOPING EACH OTHER Who said this game is a horror game but MEHHHH Lieesssss ahhahahaha xDDDI never laugh so hard cause of this game I LOVE IT &lt 3333\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_reviews = df['review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "cleaned_reviews = []\n",
    "remove_rows = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "orig_len = len(df)\n",
    "start_time = time.time()\n",
    "for i, each_review in enumerate(subset_reviews):\n",
    "    each_cleaned_review = []\n",
    "    words = each_review.split(\" \")\n",
    "    for idx, each_word in enumerate(words):\n",
    "        encoded_text = each_word.encode('utf8')\n",
    "        if \"\\\\\" in str(encoded_text):\n",
    "            remove_rows.append(i)\n",
    "            break\n",
    "        cleaned = each_word.replace(\"(\", \"\").replace(\")\", \"\") # remove ()\n",
    "        if \".\" in cleaned:\n",
    "            if cleaned.split(\".\")[1] not in [\"\", \"'\"]:\n",
    "                cleaned = cleaned.replace(\".\", \". \") # add space after \".\" \n",
    "        cleaned = cleaned.translate(table).lower()\n",
    "        cleaned = reduce_lengthening(cleaned)\n",
    "        if len(cleaned) != 0 and cleaned not in stop_words:\n",
    "            #figure out how to stop empty strings\n",
    "            cleaned = lemmatizer.lemmatize(cleaned)\n",
    "            each_cleaned_review.append(cleaned)\n",
    "    cleaned_reviews.append(\" \".join(each_cleaned_review))\n",
    "    if len(each_cleaned_review) == 0:\n",
    "        remove_rows.append(i)\n",
    "\n",
    "cleaned_reviews = [i for j, i in enumerate(cleaned_reviews) if j not in remove_rows]\n",
    "df.drop(df.index[remove_rows], inplace=True)\n",
    "print (\"Removed {} rows\".format(orig_len - len(df)))\n",
    "print (\"time took: \".format(time.time() - start_time))\n",
    "print(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(cleaned_reviews))\n",
    "print (len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes ~20 minutes to run\n",
    "#DOESNT LOOK LIKE WE ARE USING THIS?\n",
    "# unknowns = {} \n",
    "# chkr = enchant.checker.SpellChecker(\"en_EN\")\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# for i, review in enumerate(cleaned_reviews):\n",
    "#     if i % 50000 == 0:\n",
    "#         print(\"Took {} for 50000 reviews \".format(time.time() - start_time))\n",
    "#         start_time = time.time()\n",
    "#     each_cleaned_review = []\n",
    "#     words = review.split(\" \")\n",
    "#     for idx, each_word in enumerate(words):\n",
    "#         if len(each_word) == 0: # to handle double spaces\n",
    "#             continue\n",
    "#         if not chkr.check(each_word):\n",
    "#             if not chkr.check(each_word[0].upper()+each_word[1:]): # check proper nouns\n",
    "#                 if each_word not in unknowns.keys():\n",
    "#                     unknowns[each_word] = 1\n",
    "#                 else:\n",
    "#                     unknowns[each_word] = unknowns[each_word] + 1\n",
    "\n",
    "# common_unknowns = OrderedDict(sorted(unknowns.items(), key = itemgetter(1), reverse = True))\n",
    "\n",
    "# print(common_unknowns[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_unknowns = OrderedDict(sorted(unknowns.items(), key = itemgetter(1), reverse = True))\n",
    "\n",
    "# print(common_unknowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(common_unknowns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slang_dict = {}\n",
    "# with open(\"data/slangdict.txt\") as f:\n",
    "#     for line in f:\n",
    "#         slang = line.split(\"-\")\n",
    "#         if len(slang) > 1:\n",
    "#             key = slang[0].strip().translate(table).lower()\n",
    "#             val = slang[1].strip().translate(table).lower()\n",
    "#             slang_dict[key] = val\n",
    "# print(slang_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dict = {}\n",
    "for i, review in enumerate(cleaned_reviews):\n",
    "    if i% 50000 == 0:\n",
    "        print (\"For 50000, took {} seconds\".format(time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "    words = review.split(\" \")\n",
    "    for idx, each_word in enumerate(words):\n",
    "        if each_word not in word_count_dict.keys():\n",
    "            word_count_dict[each_word] = 1\n",
    "        else:\n",
    "            word_count_dict[each_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_word_count_dict = OrderedDict(sorted(word_count_dict.items(), key = itemgetter(1), reverse = True)[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ordered_word_count_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_reviews = []\n",
    "unk_counter = 0\n",
    "vocab_10k = ordered_word_count_dict.keys()\n",
    "\n",
    "start_time = time.time()\n",
    "for i, review in enumerate(cleaned_reviews):\n",
    "    if i% 50000 == 0:\n",
    "        print (\"For 50000, took {} seconds\".format(time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "    each_cleaned_review = []\n",
    "    words = review.split(\" \")\n",
    "    for idx, each_word in enumerate(words):\n",
    "        if each_word not in vocab_10k:\n",
    "            words[idx] = 'UNK'\n",
    "    final_cleaned_reviews.append((\" \".join(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For steam dataset, set count to < 10 for 15K word dictionary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell = SpellChecker(distance=1)\n",
    "# final_cleaned_reviews = []\n",
    "# unk_counter = 0\n",
    "\n",
    "# start_time = time.time()\n",
    "# for i, review in enumerate(cleaned_reviews):\n",
    "#     if i% 50000 == 0:\n",
    "#         print (\"For 50000, took {} seconds\".format(time.time() - start_time))\n",
    "#         start_time = time.time()\n",
    "#     each_cleaned_review = []\n",
    "#     words = review.split(\" \")\n",
    "#     for idx, each_word in enumerate(words):\n",
    "#         if word_count_dict[each_word] < 10:\n",
    "#             words[idx] = 'UNK'\n",
    "# #         if each_word in unknowns.keys():\n",
    "# #             if unknowns[each_word] < 10000000: #trying to cut down words\n",
    "# #                 if each_word in slang_dict.keys():\n",
    "# #                     continue\n",
    "# #                 w1 = spell.correction(each_word)\n",
    "# #                 if w1 != each_word:\n",
    "# #                     words[idx] = w1\n",
    "# #                 else:\n",
    "# #                 words[idx] = 'UNK'\n",
    "# #                 unk_counter+=1\n",
    "                    \n",
    "#     final_cleaned_reviews.append((\" \".join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out vocab size after this cleaning portion\n",
    "word_set = {}\n",
    "for each_review in final_cleaned_reviews:\n",
    "    word_list = each_review.split(\" \")\n",
    "    for each_word in word_list:\n",
    "        word_set[each_word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned vocab size\n",
    "# 1K min word count --> 38K vocab size\n",
    "# 2k min word count --> 38k vocab size\n",
    "# 10K min word count --> 30K\n",
    "\n",
    "\n",
    "len(word_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_cleaned_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_reviews'] = final_cleaned_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = test_drop['review']\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "vectors = vectorizer.fit_transform(raw_text.apply(lambda x: np.str_(x))) #.apply(lambda x: np.str_(x))\n",
    "\n",
    "# X_as_array = X.toarray()\n",
    "# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
    "# print(len(X_as_array))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense = vectors.todense()\n",
    "vectorlist = vectors.tolist()\n",
    "tf_idf = pd.DataFrame(vectorlist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors.shape)\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ben's Date Stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newColumns = df[\"date_posted\"].str.split(\"-\", n = 2, expand = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = newColumns[0]\n",
    "df['Month'] = newColumns[1]\n",
    "df['Day'] = newColumns[2]\n",
    "df.drop(columns = ['date_posted'], inplace = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding text below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = -1\n",
    "len_set = []\n",
    "for i in range(len(df)):\n",
    "    review = df.iloc[i].cleaned_reviews\n",
    "    words = review.split(\" \")\n",
    "    len_set.append(len(words))\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "len_set.sort()\n",
    "print(len_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove reviews over a limit\n",
    "max_len = -1\n",
    "removed_idx = []\n",
    "for i in range(len(df)):\n",
    "    review = df.iloc[i].cleaned_reviews\n",
    "    words = review.split(\" \")\n",
    "    if len(words) > 171:\n",
    "        removed_idx.append(i)\n",
    "\n",
    "\n",
    "df_ = df.drop(df.index[removed_idx])\n",
    "print (\"removed {} reviews > 200 words\".format(len(removed_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = None\n",
    "MAX_SEQ_LEN = 0\n",
    "\n",
    "# find vocab_size\n",
    "all_words = {}\n",
    "for each_review in df_.cleaned_reviews.tolist():\n",
    "    word_list = each_review.split(\" \")\n",
    "    # find max seq len\n",
    "    if len(word_list) > MAX_SEQ_LEN:\n",
    "        MAX_SEQ_LEN = len(word_list)\n",
    "        sent = word_list\n",
    "\n",
    "    for ea_word in word_list:\n",
    "        if ea_word in all_words:\n",
    "            all_words[ea_word] += 1\n",
    "        else:\n",
    "            all_words[ea_word] = 1\n",
    "VOCAB_SIZE = len(all_words.keys())\n",
    "print ('vocab_size = ', VOCAB_SIZE)\n",
    "print ('max_seq_len = ', MAX_SEQ_LEN)\n",
    "# print (MAX_SEQ_LEN, sent)\n",
    "# vocab_size =  52716\n",
    "# max_seq_len =  7984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_reviews = df_.cleaned_reviews.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_reviews = [one_hot(x, VOCAB_SIZE) for x in final_cleaned_reviews]\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "padded_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text_cols = padded_reviews.shape[1]\n",
    "for col_idx in range(encoded_text_cols):\n",
    "    df_.insert(len(df_.columns), \"encoded_{}\".format(col_idx+1), padded_reviews[:, col_idx])\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below is to save cleaned dataset -- don't edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_dataset(final_df):\n",
    "    # dataset must be a pd dataframe\n",
    "    root = \"data/\"\n",
    "    final_df.to_csv(root + \"cleaned_steam_data_4-15_15Kwords.csv\")\n",
    "save_cleaned_dataset(df_) # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_dataset(dataset):\n",
    "    # dataset must be a np array\n",
    "    root = \"data/\"\n",
    "    cols_added = dataset.shape[1]\n",
    "    col_names = []\n",
    "    for col_idx in range(cols_added):\n",
    "        col_names.append(\"embedded_{}\".format(col_idx))\n",
    "    dataset_df = pd.DataFrame(data=dataset, columns=col_names)\n",
    "    dataset_df.to_csv(\"steam_text_data_4-15.csv\")\n",
    "#     savetxt(root + 'cleaned_steam_data_3-28.csv', dataset, delimiter=',')\n",
    "save_cleaned_dataset(padded_reviews) # testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMAZON DATASET CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('data/reviews_Video_Games_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
