{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# keras imports\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re, itertools\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pattern.en import suggest\n",
    "\n",
    "import enchant\n",
    "\n",
    "from enchant.checker import SpellChecker\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/steam_reviews.csv\"\n",
    "df = pd.read_csv(data_dir)\n",
    "# print(type(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove nan reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = len(df)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "print ('dropped {} nan reviews'.format(orig_len - len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "len(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'@!testTEST!@#!@#'.translate(table).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up textual data (try this stuff later, do vanilla run first)\n",
    "* Remove common stopwords?\n",
    "* Lower case everything \n",
    "* All ‘s in the data is preceded by a ‘\\’ which should be cleaned out\n",
    "* Maybe don't lower case everything... ex: \n",
    "    * \"I HIGHLY RECOMMEND THIS GAME CAUSE THERE ARE SOME KILLERS WILL FOLLOW YOU NO MATTER WHERE YOU GO IN WHICH YOU'LL GO LOOPING EACH OTHER Who said this game is a horror game but MEHHHH Lieesssss ahhahahaha xDDDI never laugh so hard cause of this game I LOVE IT &lt 3333\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_reviews = df['review'][:50].tolist()\n",
    "subset_reviews = df['review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of (), so I'm cleaning it out\n",
    "# add space after \".\"\n",
    "# cleaned_reviews = []\n",
    "\n",
    "# for each_review in subset_reviews:\n",
    "#     each_cleaned_review = []\n",
    "#     words = each_review.split(\" \")\n",
    "#     for idx, each_word in enumerate(words):\n",
    "#         cleaned = each_word.replace(\"(\", \"\").replace(\")\", \"\") # remove ()\n",
    "#         if \".\" in cleaned:\n",
    "#             if cleaned.split(\".\")[1] not in [\"\", \"'\"]:\n",
    "#                 cleaned = cleaned.replace(\".\", \". \") # add space after \".\" \n",
    "#         cleaned = cleaned.translate(table).lower()\n",
    "#         each_cleaned_review.append(cleaned)\n",
    "#     cleaned_reviews.append(\" \".join(each_cleaned_review))\n",
    "# cleaned_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "\n",
    "# word = \"pls\"\n",
    "# word_wlf = reduce_lengthening(word) #calling function defined above\n",
    "# print(word_wlf) #word lengthening isn't being able to fix it completely\n",
    "\n",
    "# correct_word = suggest(word_wlf) \n",
    "# print(correct_word)\n",
    "\n",
    "# from autocorrect import Speller\n",
    "\n",
    "# spell = Speller(lang='en')\n",
    "# print(spell(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = ['&gt', 'Played', 'as', 'German', 'Reich&gt', 'Declare', 'war', 'on', 'Belgium&gt', \"Can't\", 'break', 'Belgium', 'so', 'go', 'through', 'France&gt', 'Capitulate', 'France', 'in', 'order', 'to', 'get', 'to', 'Belgium&gt', 'Get', 'True', 'Blitzkrieg', 'achievementThis', 'game', 'is', 'dad']\n",
    "# for idx, each_word in enumerate(words):\n",
    "#     print(each_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter    \n",
    "\n",
    "cleaned_reviews = []\n",
    "remove_rows = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "for i, each_review in enumerate(subset_reviews):\n",
    "    each_cleaned_review = []\n",
    "    words = each_review.split(\" \")\n",
    "    for idx, each_word in enumerate(words):\n",
    "        encoded_text = each_word.encode('utf8')\n",
    "        if \"\\\\\" in str(encoded_text):\n",
    "            remove_rows.append(idx)\n",
    "            break\n",
    "        cleaned = each_word.replace(\"(\", \"\").replace(\")\", \"\") # remove ()\n",
    "        if \".\" in cleaned:\n",
    "            if cleaned.split(\".\")[1] not in [\"\", \"'\"]:\n",
    "                cleaned = cleaned.replace(\".\", \". \") # add space after \".\" \n",
    "        cleaned = cleaned.translate(table).lower()\n",
    "        cleaned = reduce_lengthening(cleaned)\n",
    "        if len(cleaned) != 0 and cleaned not in stop_words:\n",
    "            #figure out how to stop empty strings\n",
    "            each_cleaned_review.append(cleaned)#.replace(\" \", \"\"))\n",
    "        else:\n",
    "            remove_rows.append(i) \n",
    "            continue\n",
    "    if len(each_cleaned_review) > 0:\n",
    "        cleaned_reviews.append(\" \".join(each_cleaned_review))\n",
    "# df.drop(df.index[remove_rows])\n",
    "# print(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_reviews[4775])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknowns = {}\n",
    "\n",
    "chkr = enchant.checker.SpellChecker(\"en_EN\")\n",
    "\n",
    "for i, review in enumerate(cleaned_reviews):\n",
    "    each_cleaned_review = []\n",
    "    words = review.split(\" \")\n",
    "    for idx, each_word in enumerate(words):\n",
    "        if len(each_word) == 0:\n",
    "            continue\n",
    "        if not chkr.check(each_word):\n",
    "            if not chkr.check(each_word[0].upper()+each_word[1:]):\n",
    "                if each_word not in unknowns.keys():\n",
    "                    unknowns[each_word] = 1\n",
    "                else:\n",
    "                    if \n",
    "                        \n",
    "                    else:\n",
    "                        unknowns[each_word] = unknowns[each_word] + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_unknowns = OrderedDict(sorted(unknowns.items(), key = itemgetter(1), reverse = True))\n",
    "\n",
    "print(common_unknowns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_wlf = 'pervect'\n",
    "\n",
    "print(chkr.check(word_wlf))\n",
    "\n",
    "correct_word = suggest(word_wlf) \n",
    "print(correct_word)\n",
    "print(correct_word[0][1])\n",
    "\n",
    "# from autocorrect import Speller\n",
    "\n",
    "# spells = Speller(lang='en')\n",
    "# print(spells(word_wlf))\n",
    "\n",
    "spell = SpellChecker(distance=1)\n",
    "print(list(spell.unknown([word_wlf])))\n",
    "print(list(spell.known([word_wlf])))\n",
    "w = list(spell.unknown([word_wlf]))[0]\n",
    "print(spell.correction(w))\n",
    "print(spell.candidates(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {}\n",
    "with open(\"data/slangdict.txt\") as f:\n",
    "    for line in f:\n",
    "        slang = line.split(\"-\")\n",
    "        if len(slang) > 1:\n",
    "            key = slang[0].strip().translate(table).lower()\n",
    "            val = slang[1].strip().translate(table).lower()\n",
    "            slang_dict[key] = val\n",
    "print(slang_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker(distance=1)\n",
    "\n",
    "for i, review in enumerate(cleaned_reviews):\n",
    "    each_cleaned_review = []\n",
    "    words = review.split(\" \")\n",
    "    for idx, each_word in enumerate(words):\n",
    "        if each_word in unknowns.keys():\n",
    "            if unknowns[each_word] >= 500:\n",
    "                continue\n",
    "            else:\n",
    "                if each_word in slang_dict.keys():\n",
    "    #                  words[idx] =slang_dict[each_word]:\n",
    "                        continue\n",
    "                else:\n",
    "                    if unknowns[each_word] < 500:\n",
    "                        fix = suggest(each_word)\n",
    "                        w1 = fix[0][0]\n",
    "#                         prob = fix[0][1]\n",
    "#                         print(list(spell.unknown([word_wlf])))\n",
    "#                         print(list(spell.known([word_wlf])))\n",
    "#                         w = list(spell.unknown([word_wlf]))[0]\n",
    "                        w2 = spell.correction(each_word)\n",
    "                        if w1 == w2:\n",
    "                            words[idx] = w1\n",
    "                        else:\n",
    "                            words[idx] = 'UNK'\n",
    "    cleaned_reviews[i] = (\" \".join(words))\n",
    "# print(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#todo spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['is_early_access_review'] = df['is_early_access_review'].astype('int')\n",
    "# df['recommendation'] = df['recommendation'].map({'Recommended':1, 'Not Recommended':0})\n",
    "# df = pd.get_dummies(df, columns = ['title'] )\n",
    "df['cleaned_reviews'] = cleaned_reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out foreign language and bold/italics, etc\n",
    "# reviews = df['cleaned_reviews'].tolist()\n",
    "# reviews = df['review'].tolist()\n",
    "# deleted_rows = 0\n",
    "# bad_rows = []\n",
    "\n",
    "# for idx, eachreview in enumerate(reviews):\n",
    "#     word_list = eachreview.split(\" \")\n",
    "#     for eachword in word_list:\n",
    "#         encoded_text = eachword.encode('utf8')\n",
    "#         if \"\\\\\" in str(encoded_text):\n",
    "#             bad_rows.append(idx)\n",
    "#             break\n",
    "\n",
    "\n",
    "# test_drop = df.drop(df.index[bad_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) - len(test_drop)\n",
    "# dropped less, look into this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = test_drop['review']\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "vectors = vectorizer.fit_transform(raw_text.apply(lambda x: np.str_(x))) #.apply(lambda x: np.str_(x))\n",
    "\n",
    "# X_as_array = X.toarray()\n",
    "# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
    "# print(len(X_as_array))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense = vectors.todense()\n",
    "vectorlist = vectors.tolist()\n",
    "tf_idf = pd.DataFrame(vectorlist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors.shape)\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ben's Date Stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newColumns = df[\"date_posted\"].str.split(\"-\", n = 2, expand = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = newColumns[0]\n",
    "df['Month'] = newColumns[1]\n",
    "df['Day'] = newColumns[2]\n",
    "df.drop(columns = ['date_posted'], inplace = True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding text below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = None\n",
    "MAX_SEQ_LEN = 0\n",
    "\n",
    "# find vocab_size\n",
    "all_words = {}\n",
    "for each_review in cleaned_reviews:\n",
    "    word_list = each_review.split(\" \")\n",
    "    # find max seq len\n",
    "    if len(word_list) > MAX_SEQ_LEN:\n",
    "        MAX_SEQ_LEN = len(word_list)\n",
    "        sent = word_list\n",
    "\n",
    "    for ea_word in word_list:\n",
    "        if ea_word in all_words:\n",
    "            all_words[ea_word] += 1\n",
    "        else:\n",
    "            all_words[ea_word] = 1\n",
    "VOCAB_SIZE = len(all_words.keys())\n",
    "print ('vocab_size = ', VOCAB_SIZE)\n",
    "print ('max_seq_len = ', MAX_SEQ_LEN)\n",
    "# print (MAX_SEQ_LEN, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_reviews = [one_hot(x, VOCAB_SIZE) for x in cleaned_reviews]\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=MAX_SEQ_LEN, padding='pre')\n",
    "padded_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below is to save cleaned dataset -- don't edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_dataset(dataset):\n",
    "    # dataset must be a np array\n",
    "    root = \"data/\"\n",
    "    savetxt(root + 'cleaned_steam_data.csv', dataset, delimiter=',')\n",
    "save_cleaned_dataset(padded_docs) # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
