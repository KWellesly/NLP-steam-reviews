{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "GeForce GTX 1060 with Max-Q Design\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print (torch.cuda.is_available())\n",
    "print (torch.cuda.current_device())\n",
    "print (torch.cuda.get_device_name(0))\n",
    "print (torch.cuda.memory_allocated())\n",
    "print (torch.cuda.memory_cached())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned_steam_data_4-15_15Kwords.csv\", encoding='utf8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommendation</th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>encoded_6</th>\n",
       "      <th>encoded_7</th>\n",
       "      <th>encoded_8</th>\n",
       "      <th>encoded_9</th>\n",
       "      <th>...</th>\n",
       "      <th>encoded_185</th>\n",
       "      <th>encoded_186</th>\n",
       "      <th>encoded_187</th>\n",
       "      <th>encoded_188</th>\n",
       "      <th>encoded_189</th>\n",
       "      <th>encoded_190</th>\n",
       "      <th>encoded_191</th>\n",
       "      <th>encoded_192</th>\n",
       "      <th>encoded_193</th>\n",
       "      <th>encoded_194</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5883</td>\n",
       "      <td>8055</td>\n",
       "      <td>5987</td>\n",
       "      <td>3547</td>\n",
       "      <td>5987</td>\n",
       "      <td>5030</td>\n",
       "      <td>3547</td>\n",
       "      <td>3547</td>\n",
       "      <td>11106</td>\n",
       "      <td>5315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11106</td>\n",
       "      <td>3090</td>\n",
       "      <td>13779</td>\n",
       "      <td>7175</td>\n",
       "      <td>7891</td>\n",
       "      <td>1064</td>\n",
       "      <td>3380</td>\n",
       "      <td>1917</td>\n",
       "      <td>5409</td>\n",
       "      <td>3118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13408</td>\n",
       "      <td>11246</td>\n",
       "      <td>9544</td>\n",
       "      <td>6270</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11132</td>\n",
       "      <td>13289</td>\n",
       "      <td>12336</td>\n",
       "      <td>8729</td>\n",
       "      <td>12154</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   recommendation  encoded_1  encoded_2  encoded_3  encoded_4  encoded_5  \\\n",
       "0               1          0          0          0          0          0   \n",
       "1               1          0          0          0          0          0   \n",
       "2               1          0          0          0          0          0   \n",
       "7               1          0          0          0          0          0   \n",
       "9               1          0          0          0          0          0   \n",
       "\n",
       "   encoded_6  encoded_7  encoded_8  encoded_9  ...  encoded_185  encoded_186  \\\n",
       "0          0          0          0          0  ...         5883         8055   \n",
       "1          0          0          0          0  ...            0            0   \n",
       "2          0          0          0          0  ...        11106         3090   \n",
       "7          0          0          0          0  ...            0            0   \n",
       "9          0          0          0          0  ...            0            0   \n",
       "\n",
       "   encoded_187  encoded_188  encoded_189  encoded_190  encoded_191  \\\n",
       "0         5987         3547         5987         5030         3547   \n",
       "1            0            0            0            0            0   \n",
       "2        13779         7175         7891         1064         3380   \n",
       "7            0            0            0        13408        11246   \n",
       "9            0            0        11132        13289        12336   \n",
       "\n",
       "   encoded_192  encoded_193  encoded_194  \n",
       "0         3547        11106         5315  \n",
       "1            0            0         9665  \n",
       "2         1917         5409         3118  \n",
       "7         9544         6270        11106  \n",
       "9         8729        12154        11106  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_columns = [x for x in df.columns.tolist() if x.startswith('title_')]\n",
    "drop_cols = ['funny', 'is_early_access_review', 'helpful', 'review', 'cleaned_reviews', 'hour_played', 'Year', 'Month', 'Day']\n",
    "drop_cols += title_columns\n",
    "\n",
    "df.drop(drop_cols, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = len(df.columns.tolist())-1\n",
    "VOCAB_SIZE = 400000# 14845 # should ideally just transport this from prev\n",
    "EMBED_DIM = 100\n",
    "LSTM_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need 2-3 lines for attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pytorch model\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "class Attention_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention_Net, self).__init__()\n",
    "        \n",
    "        # define architecture\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM) # add pretrained embeding\n",
    "        \n",
    "        self.lstm = nn.LSTM(EMBED_DIM, \n",
    "                            LSTM_DIM, \n",
    "                            bidirectional=True,\n",
    "                            dropout=0.2,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # attention layer\n",
    "#         self.attention_layer = Attention(LSTM_DIM * 2, MAX_SEQ_LEN)\n",
    "        # try tanh\n",
    "\n",
    "        self.linear = nn.Linear(LSTM_DIM*2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "        embedding = torch.squeeze(torch.unsqueeze(embedding, 0)).view(BATCH_SIZE, MAX_SEQ_LEN, -1)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedding)\n",
    "#         attention = self.attention_layer(lstm_out)\n",
    "        \n",
    "        out = self.linear(lstm_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteamDataset(data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        #'Initialization'\n",
    "        self.data = data\n",
    "        text_cols = [x for x in df.columns.tolist() if x.startswith(\"encoded\")]\n",
    "        self.train = torch.tensor(data[text_cols].values).type(torch.LongTensor).cuda()\n",
    "        labels = data['recommendation'].tolist()\n",
    "        \n",
    "        self.one_hot_labels = torch.tensor(np.array(labels)).squeeze().type(torch.LongTensor).cuda() # change to longtensor if using custom loss\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        #'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #'Generates one sample of data'\n",
    "        \n",
    "        # Load data and get label\n",
    "        X = self.train[index]\n",
    "        Y = self.one_hot_labels[index]\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1f441d93048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num = int(0.8 * len(df))\n",
    "steam_dataset = SteamDataset(df[:train_num])\n",
    "steam_data_loader = data.DataLoader(steam_dataset, batch_size=BATCH_SIZE, num_workers=0, drop_last=True, shuffle=True)\n",
    "steam_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "PATH = 'models/amzn_date4-16_batch128_epoch10_acc83_lstm64_pretrainedembedding_noattention.pt'\n",
    "attention_model = Attention_Net().cuda()\n",
    "attention_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(attention_model.parameters(), lr=0.0001) # even lower for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8646136093194e989da1588858622827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | took 121.9320011138916 seconds | summed loss: 812.6401977539062 | avg loss: 0.0025044383946806192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119a02d2c84747e2812308c10e8e0802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | took 122.2736930847168 seconds | summed loss: 761.7395629882812 | avg loss: 0.0023475701455026865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebb260d306d429b8d4a9caf7932a006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | took 122.77803039550781 seconds | summed loss: 736.038330078125 | avg loss: 0.002268362557515502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533b80a1f7f44df5b9721f9ca2a5c489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | took 122.51658225059509 seconds | summed loss: 719.1849365234375 | avg loss: 0.0022164229303598404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56aba449a04d444592244bfc78c2452c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | took 122.5858268737793 seconds | summed loss: 705.7792358398438 | avg loss: 0.002175108529627323\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dab13a8fb534f1fb6e2f1d9a2a8c99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | took 122.46696543693542 seconds | summed loss: 694.8030395507812 | avg loss: 0.002141281496733427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd01ac15d304821ab705e04c7f36066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | took 123.15779399871826 seconds | summed loss: 685.0997314453125 | avg loss: 0.002111377427354455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2916c70306495182e2bfbb6b120cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | took 122.80027651786804 seconds | summed loss: 676.7860107421875 | avg loss: 0.002085755579173565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0b2b689d9f4745aef2dc410473d65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | took 123.1782054901123 seconds | summed loss: 668.8837890625 | avg loss: 0.002061402192339301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febb6f47eb4b4a3ab0b27712a32e7735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | took 123.05745482444763 seconds | summed loss: 661.5810546875 | avg loss: 0.002038896083831787\n",
      "Took 1226.7566776275635 seconds\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "EPOCHS = 10\n",
    "start = time.time()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    second_start = time.time()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    attention_model.train()\n",
    "    \n",
    "    with tqdm(total=len(steam_data_loader), file=sys.stdout) as pbar:\n",
    "        for idx, (train_X, train_Y) in enumerate(steam_data_loader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred_y = attention_model(train_X) \n",
    "            loss = loss_function(pred_y, train_Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss\n",
    "\n",
    "            # calc accuracy\n",
    "            pred1_mask = pred_y[:, 1] > 0.5\n",
    "            masked_trainY_1 = train_Y[pred1_mask]\n",
    "            masked_trainY_0 = train_Y[~pred1_mask]\n",
    "            ones_predicted_correct = torch.sum(masked_trainY_1)\n",
    "            zeros_predicted_correct = torch.sum(masked_trainY_0)\n",
    "            correct += ones_predicted_correct.add(zeros_predicted_correct)\n",
    "            correct_ = correct.cpu().numpy()\n",
    "            \n",
    "            # update progress bar\n",
    "            pbar.set_description('ep{} | loss: {} | acc: {}%'.format(i+1, torch.round(running_loss), round(correct_ / ((idx+1) * BATCH_SIZE)*100, 1)))\n",
    "            pbar.update(1)\n",
    "            tqdm._instances.clear()\n",
    "            \n",
    "            \n",
    "\n",
    "    print ('Epoch {} | took {} seconds | summed loss: {} | avg loss: {}'\n",
    "                   .format(i+1, time.time() - second_start, running_loss, running_loss / (len(steam_data_loader) * BATCH_SIZE)))\n",
    "\n",
    "print (\"Took {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention_Net(\n",
      "  (embedding): Embedding(400000, 100)\n",
      "  (lstm): LSTM(100, 64, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (attention_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_eval_dataset = SteamDataset(df[train_num:])\n",
    "steam_eval_data_loader = data.DataLoader(steam_eval_dataset, batch_size=BATCH_SIZE, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy: 0.8626839549387433\n",
      "Eval summed loss: 288.14398193359375 | avg loss: 0.0035514580085873604\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "correct = 0\n",
    "eval_loss = 0\n",
    "attention_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (test_X, test_Y) in enumerate(steam_eval_data_loader):\n",
    "        preds = attention_model(test_X).squeeze()\n",
    "        preds = torch.nn.functional.softmax(preds)\n",
    "        for idx, each_pred in enumerate(preds):\n",
    "            if each_pred[0] >= 0.5 and test_Y[idx] == 0:\n",
    "                correct += 1\n",
    "            elif each_pred[0] < 0.5 and test_Y[idx] == 1:\n",
    "                correct += 1\n",
    "        loss = loss_function(preds, test_Y)\n",
    "        eval_loss += loss\n",
    "        \n",
    "        \n",
    "print (\"Eval accuracy: {}\".format(correct / len(steam_eval_dataset)))\n",
    "print (\"Eval summed loss: {} | avg loss: {}\".format(eval_loss, eval_loss / len(steam_eval_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'models/transfer_learning_date4-16_acc86_epoch10_batch128.pt'\n",
    "torch.save(attention_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 77.8% eval acc -- batch=128, lr=0.0001\n",
    "# 77.8% eval acc -- batch=128, lr=0.001\n",
    "# 77.9% eval acc -- batch=64, lr=0.001, embed_dim=16, lstm_dim=16\n",
    "# 85.9% eval acc -- batch=128, lr=0.0001, embed=128, lstm_dim=64\n",
    "# 86.1% eval acc -- batch=128, lr=0.0001, embed=128, lstm_dim=64, + attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratchpaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5980,  0.9168,  0.8958], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
