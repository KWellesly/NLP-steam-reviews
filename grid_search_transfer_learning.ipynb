{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "GeForce GTX 1060 with Max-Q Design\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print (torch.cuda.is_available())\n",
    "print (torch.cuda.current_device())\n",
    "print (torch.cuda.get_device_name(0))\n",
    "print (torch.cuda.memory_allocated())\n",
    "print (torch.cuda.memory_cached())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned_steam_data_4-15_15Kwords.csv\", encoding='utf8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommendation</th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>encoded_6</th>\n",
       "      <th>encoded_7</th>\n",
       "      <th>encoded_8</th>\n",
       "      <th>encoded_9</th>\n",
       "      <th>...</th>\n",
       "      <th>encoded_185</th>\n",
       "      <th>encoded_186</th>\n",
       "      <th>encoded_187</th>\n",
       "      <th>encoded_188</th>\n",
       "      <th>encoded_189</th>\n",
       "      <th>encoded_190</th>\n",
       "      <th>encoded_191</th>\n",
       "      <th>encoded_192</th>\n",
       "      <th>encoded_193</th>\n",
       "      <th>encoded_194</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5883</td>\n",
       "      <td>8055</td>\n",
       "      <td>5987</td>\n",
       "      <td>3547</td>\n",
       "      <td>5987</td>\n",
       "      <td>5030</td>\n",
       "      <td>3547</td>\n",
       "      <td>3547</td>\n",
       "      <td>11106</td>\n",
       "      <td>5315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11106</td>\n",
       "      <td>3090</td>\n",
       "      <td>13779</td>\n",
       "      <td>7175</td>\n",
       "      <td>7891</td>\n",
       "      <td>1064</td>\n",
       "      <td>3380</td>\n",
       "      <td>1917</td>\n",
       "      <td>5409</td>\n",
       "      <td>3118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13408</td>\n",
       "      <td>11246</td>\n",
       "      <td>9544</td>\n",
       "      <td>6270</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11132</td>\n",
       "      <td>13289</td>\n",
       "      <td>12336</td>\n",
       "      <td>8729</td>\n",
       "      <td>12154</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   recommendation  encoded_1  encoded_2  encoded_3  encoded_4  encoded_5  \\\n",
       "0               1          0          0          0          0          0   \n",
       "1               1          0          0          0          0          0   \n",
       "2               1          0          0          0          0          0   \n",
       "7               1          0          0          0          0          0   \n",
       "9               1          0          0          0          0          0   \n",
       "\n",
       "   encoded_6  encoded_7  encoded_8  encoded_9  ...  encoded_185  encoded_186  \\\n",
       "0          0          0          0          0  ...         5883         8055   \n",
       "1          0          0          0          0  ...            0            0   \n",
       "2          0          0          0          0  ...        11106         3090   \n",
       "7          0          0          0          0  ...            0            0   \n",
       "9          0          0          0          0  ...            0            0   \n",
       "\n",
       "   encoded_187  encoded_188  encoded_189  encoded_190  encoded_191  \\\n",
       "0         5987         3547         5987         5030         3547   \n",
       "1            0            0            0            0            0   \n",
       "2        13779         7175         7891         1064         3380   \n",
       "7            0            0            0        13408        11246   \n",
       "9            0            0        11132        13289        12336   \n",
       "\n",
       "   encoded_192  encoded_193  encoded_194  \n",
       "0         3547        11106         5315  \n",
       "1            0            0         9665  \n",
       "2         1917         5409         3118  \n",
       "7         9544         6270        11106  \n",
       "9         8729        12154        11106  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_columns = [x for x in df.columns.tolist() if x.startswith('title_')]\n",
    "drop_cols = ['funny', 'is_early_access_review', 'helpful', 'review', 'cleaned_reviews', 'hour_played', 'Year', 'Month', 'Day']\n",
    "drop_cols += title_columns\n",
    "\n",
    "df.drop(drop_cols, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES_LIST = [0.0001]\n",
    "EPOCHS_LIST = [10, 20, 30, 40, 50]\n",
    "BATCH_SIZES_LIST = [64, 128]\n",
    "LSTM_DIMS_LIST = [64]\n",
    "EMBED_DIMS_LIST = [100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storage for best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_model_loss_arr = []\n",
    "highest_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = len(df.columns.tolist())-1\n",
    "VOCAB_SIZE = 400000# 14845 # should ideally just transport this from prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need 2-3 lines for attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Net(nn.Module):\n",
    "    def __init__(self, EMBED_DIM, LSTM_DIM, VOCAB_SIZE, BATCH_SIZE):\n",
    "        super(Attention_Net, self).__init__()\n",
    "        \n",
    "\n",
    "        # define architecture\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM) # add pretrained embeding\n",
    "\n",
    "        self.lstm = nn.LSTM(EMBED_DIM, \n",
    "                            LSTM_DIM, \n",
    "                            bidirectional=True,\n",
    "                            dropout=0.2,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # attention layer\n",
    "        self.attention_layer = Attention(LSTM_DIM * 2, MAX_SEQ_LEN)\n",
    "        # try tanh\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.linear = nn.Linear(LSTM_DIM*2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "        embedding = torch.squeeze(torch.unsqueeze(embedding, 0)).view(BATCH_SIZE, MAX_SEQ_LEN, -1)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedding)\n",
    "        attention = self.attention_layer(lstm_out)\n",
    "        tanh = self.tanh(attention)\n",
    "\n",
    "        # out = self.linear(lstm_out[:, -1, :])\n",
    "        out = self.linear(tanh)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteamDataset(data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        #'Initialization'\n",
    "        self.data = data\n",
    "        text_cols = [x for x in df.columns.tolist() if x.startswith(\"encoded\")]\n",
    "        self.train = torch.tensor(data[text_cols].values).type(torch.LongTensor).cuda()\n",
    "        labels = data['recommendation'].tolist()\n",
    "\n",
    "        self.one_hot_labels = torch.tensor(np.array(labels)).squeeze().type(torch.LongTensor).cuda() # change to longtensor if using custom loss\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #'Generates one sample of data'\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.train[index]\n",
    "        Y = self.one_hot_labels[index]\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f9d6ef84954c3d9362b43721086a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1267.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | took 85.36829662322998 seconds | summed loss: 592.16845703125 | avg loss: 0.001825696905143559\n",
      "Took 85.36926102638245 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy: 0.8090566223777947\n",
      "Eval summed loss: 162.75315856933594 | avg loss: 0.002005979884415865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ed3ef8aab84600acdd8de687967f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1622.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | took 97.80365061759949 seconds | summed loss: 753.0336303710938 | avg loss: 0.0023213119711726904\n",
      "Took 97.80462527275085 seconds\n",
      "Eval accuracy: 0.8082554785909729\n",
      "Eval summed loss: 219.3035125732422 | avg loss: 0.0027029793709516525\n"
     ]
    }
   ],
   "source": [
    "# build pytorch model\n",
    "DROPOUT = 0.1\n",
    "for LEARNING_RATE in LEARNING_RATES_LIST:\n",
    "    for EPOCHS in EPOCHS_LIST:\n",
    "        for EMBED_DIM in EMBED_DIMS_LIST:\n",
    "            for LSTM_DIM in LSTM_DIMS_LIST:\n",
    "                for BATCH_SIZE in BATCH_SIZES_LIST:\n",
    "\n",
    "                    train_num = int(0.8 * len(df))\n",
    "                    steam_dataset = SteamDataset(df[:train_num])\n",
    "                    steam_data_loader = data.DataLoader(steam_dataset, batch_size=BATCH_SIZE, num_workers=0, drop_last=True, shuffle=True)\n",
    "\n",
    "                    PATH = 'models/amzn_test_attention_transferlearning.pt'\n",
    "                    attention_model = Attention_Net(EMBED_DIM, LSTM_DIM, VOCAB_SIZE, BATCH_SIZE).cuda()\n",
    "                    attention_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "                    loss_function = nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.Adam(attention_model.parameters(), lr=LEARNING_RATE) # even lower for transfer learning\n",
    "\n",
    "                    # training loop\n",
    "                    start = time.time()\n",
    "                    \n",
    "                    loss_arr = []\n",
    "\n",
    "                    for i in range(EPOCHS):\n",
    "                        second_start = time.time()\n",
    "                        running_loss = 0\n",
    "                        correct = 0\n",
    "                        attention_model.train()\n",
    "\n",
    "                        with tqdm(total=len(steam_data_loader), file=sys.stdout) as pbar:\n",
    "                            for idx, (train_X, train_Y) in enumerate(steam_data_loader):\n",
    "\n",
    "                                optimizer.zero_grad()\n",
    "\n",
    "                                pred_y = attention_model(train_X) \n",
    "                                loss = loss_function(pred_y, train_Y)\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "                                running_loss += loss\n",
    "                                loss_arr.append(running_loss)\n",
    "\n",
    "#                                 # calc accuracy\n",
    "#                                 pred1_mask = pred_y[:, 1] > 0.5\n",
    "#                                 masked_trainY_1 = train_Y[pred1_mask]\n",
    "#                                 masked_trainY_0 = train_Y[~pred1_mask]\n",
    "#                                 ones_predicted_correct = torch.sum(masked_trainY_1)\n",
    "#                                 zeros_predicted_correct = torch.sum(masked_trainY_0)\n",
    "#                                 correct += ones_predicted_correct.add(zeros_predicted_correct)\n",
    "#                                 correct_ = correct.cpu().numpy()\n",
    "\n",
    "                                # update progress bar\n",
    "                                pbar.set_description('ep{} | loss: {}'.format(i+1, torch.round(running_loss)))\n",
    "                                pbar.update(1)\n",
    "                                tqdm._instances.clear()\n",
    "\n",
    "\n",
    "\n",
    "                        print ('Epoch {} | took {} seconds | summed loss: {} | avg loss: {}'\n",
    "                                       .format(i+1, time.time() - second_start, running_loss, running_loss / (len(steam_data_loader) * BATCH_SIZE)))\n",
    "\n",
    "                    print (\"Took {} seconds\".format(time.time() - start))\n",
    "\n",
    "\n",
    "                    steam_eval_dataset = SteamDataset(df[train_num:])\n",
    "                    steam_eval_data_loader = data.DataLoader(steam_eval_dataset, batch_size=BATCH_SIZE, num_workers=0, drop_last=True)\n",
    "\n",
    "                    # evaluate\n",
    "                    correct = 0\n",
    "                    eval_loss = 0\n",
    "                    attention_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for i, (test_X, test_Y) in enumerate(steam_eval_data_loader):\n",
    "                            preds = attention_model(test_X).squeeze()\n",
    "                            preds = torch.nn.functional.softmax(preds)\n",
    "                            for idx, each_pred in enumerate(preds):\n",
    "                                if each_pred[0] >= 0.5 and test_Y[idx] == 0:\n",
    "                                    correct += 1\n",
    "                                elif each_pred[0] < 0.5 and test_Y[idx] == 1:\n",
    "                                    correct += 1\n",
    "                            loss = loss_function(preds, test_Y)\n",
    "                            eval_loss += loss\n",
    "                            \n",
    "                    accuracy = correct / len(steam_eval_dataset)\n",
    "                    if accuracy > highest_acc:\n",
    "                        highest_acc = accuracy\n",
    "                        best_model = attention_model.state_dict()\n",
    "                        best_model_loss_arr = loss_arr\n",
    "                        best_batch = BATCH_SIZE\n",
    "                        best_lstm_dim = LSTM_DIM\n",
    "                        best_epochs = EPOCHS\n",
    "                        best_lr = LEARNING_RATE\n",
    "\n",
    "\n",
    "                    print (\"Eval accuracy: {}\".format(correct / len(steam_eval_dataset)))\n",
    "                    print (\"Eval summed loss: {} | avg loss: {}\".format(eval_loss, eval_loss / len(steam_eval_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8090566223777947\n"
     ]
    }
   ],
   "source": [
    "PATH = 'models/transfer_learning_grid_search_date4-21_batch' + str(best_batch) + '_epoch' + str(best_epochs) + '_lstm' + str(best_lstm_dim) + '_lr' + str(best_lr) + '.pt'\n",
    "print(highest_acc)\n",
    "torch.save(best_model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Graph Over Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_arr = []\n",
    "for i in range(50):\n",
    "    epochs_arr.append(i+1)\n",
    "    \n",
    "losses = np.array([x.cpu().detach().numpy() for x in best_model_loss_arr])\n",
    "np.save('data/transferlearning_y_axis', losses)\n",
    "    \n",
    "plt.plot(epochs_arr, best_model_loss_arr)\n",
    "plt.ylabel(\"Total Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
