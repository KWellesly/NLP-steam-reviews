{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "GeForce GTX 1060 with Max-Q Design\n",
      "732639744\n",
      "794820608\n"
     ]
    }
   ],
   "source": [
    "print (torch.cuda.is_available())\n",
    "print (torch.cuda.current_device())\n",
    "print (torch.cuda.get_device_name(0))\n",
    "print (torch.cuda.memory_allocated())\n",
    "print (torch.cuda.memory_cached())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned_steam_data_3-29.csv\", encoding='utf8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommendation</th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>encoded_6</th>\n",
       "      <th>encoded_7</th>\n",
       "      <th>encoded_8</th>\n",
       "      <th>encoded_9</th>\n",
       "      <th>...</th>\n",
       "      <th>encoded_185</th>\n",
       "      <th>encoded_186</th>\n",
       "      <th>encoded_187</th>\n",
       "      <th>encoded_188</th>\n",
       "      <th>encoded_189</th>\n",
       "      <th>encoded_190</th>\n",
       "      <th>encoded_191</th>\n",
       "      <th>encoded_192</th>\n",
       "      <th>encoded_193</th>\n",
       "      <th>encoded_194</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33572</td>\n",
       "      <td>15335</td>\n",
       "      <td>18484</td>\n",
       "      <td>25557</td>\n",
       "      <td>18484</td>\n",
       "      <td>12429</td>\n",
       "      <td>8541</td>\n",
       "      <td>25557</td>\n",
       "      <td>14122</td>\n",
       "      <td>23610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14122</td>\n",
       "      <td>39490</td>\n",
       "      <td>27271</td>\n",
       "      <td>39764</td>\n",
       "      <td>28167</td>\n",
       "      <td>8875</td>\n",
       "      <td>1494</td>\n",
       "      <td>40182</td>\n",
       "      <td>6951</td>\n",
       "      <td>39745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34155</td>\n",
       "      <td>14517</td>\n",
       "      <td>19363</td>\n",
       "      <td>40032</td>\n",
       "      <td>14122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10818</td>\n",
       "      <td>2776</td>\n",
       "      <td>9421</td>\n",
       "      <td>19118</td>\n",
       "      <td>3218</td>\n",
       "      <td>14122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   recommendation  encoded_1  encoded_2  encoded_3  encoded_4  encoded_5  \\\n",
       "0               1          0          0          0          0          0   \n",
       "1               1          0          0          0          0          0   \n",
       "2               1          0          0          0          0          0   \n",
       "7               1          0          0          0          0          0   \n",
       "9               1          0          0          0          0          0   \n",
       "\n",
       "   encoded_6  encoded_7  encoded_8  encoded_9  ...  encoded_185  encoded_186  \\\n",
       "0          0          0          0          0  ...        33572        15335   \n",
       "1          0          0          0          0  ...            0            0   \n",
       "2          0          0          0          0  ...        14122        39490   \n",
       "7          0          0          0          0  ...            0            0   \n",
       "9          0          0          0          0  ...            0            0   \n",
       "\n",
       "   encoded_187  encoded_188  encoded_189  encoded_190  encoded_191  \\\n",
       "0        18484        25557        18484        12429         8541   \n",
       "1            0            0            0            0            0   \n",
       "2        27271        39764        28167         8875         1494   \n",
       "7            0            0            0        34155        14517   \n",
       "9            0            0        10818         2776         9421   \n",
       "\n",
       "   encoded_192  encoded_193  encoded_194  \n",
       "0        25557        14122        23610  \n",
       "1            0            0        30552  \n",
       "2        40182         6951        39745  \n",
       "7        19363        40032        14122  \n",
       "9        19118         3218        14122  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_columns = [x for x in df.columns.tolist() if x.startswith('title_')]\n",
    "drop_cols = ['funny', 'is_early_access_review', 'helpful', 'review', 'cleaned_reviews', 'hour_played', 'Year', 'Month', 'Day']\n",
    "drop_cols += title_columns\n",
    "\n",
    "df.drop(drop_cols, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>encoded_6</th>\n",
       "      <th>encoded_7</th>\n",
       "      <th>encoded_8</th>\n",
       "      <th>encoded_9</th>\n",
       "      <th>encoded_10</th>\n",
       "      <th>...</th>\n",
       "      <th>encoded_186</th>\n",
       "      <th>encoded_187</th>\n",
       "      <th>encoded_188</th>\n",
       "      <th>encoded_189</th>\n",
       "      <th>encoded_190</th>\n",
       "      <th>encoded_191</th>\n",
       "      <th>encoded_192</th>\n",
       "      <th>encoded_193</th>\n",
       "      <th>encoded_194</th>\n",
       "      <th>helpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>15335</td>\n",
       "      <td>18484</td>\n",
       "      <td>25557</td>\n",
       "      <td>18484</td>\n",
       "      <td>12429</td>\n",
       "      <td>8541</td>\n",
       "      <td>25557</td>\n",
       "      <td>14122</td>\n",
       "      <td>23610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39490</td>\n",
       "      <td>27271</td>\n",
       "      <td>39764</td>\n",
       "      <td>28167</td>\n",
       "      <td>8875</td>\n",
       "      <td>1494</td>\n",
       "      <td>40182</td>\n",
       "      <td>6951</td>\n",
       "      <td>39745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34155</td>\n",
       "      <td>14517</td>\n",
       "      <td>19363</td>\n",
       "      <td>40032</td>\n",
       "      <td>14122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10818</td>\n",
       "      <td>2776</td>\n",
       "      <td>9421</td>\n",
       "      <td>19118</td>\n",
       "      <td>3218</td>\n",
       "      <td>14122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   encoded_1  encoded_2  encoded_3  encoded_4  encoded_5  encoded_6  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "7          0          0          0          0          0          0   \n",
       "9          0          0          0          0          0          0   \n",
       "\n",
       "   encoded_7  encoded_8  encoded_9  encoded_10  ...  encoded_186  encoded_187  \\\n",
       "0          0          0          0           0  ...        15335        18484   \n",
       "1          0          0          0           0  ...            0            0   \n",
       "2          0          0          0           0  ...        39490        27271   \n",
       "7          0          0          0           0  ...            0            0   \n",
       "9          0          0          0           0  ...            0            0   \n",
       "\n",
       "   encoded_188  encoded_189  encoded_190  encoded_191  encoded_192  \\\n",
       "0        25557        18484        12429         8541        25557   \n",
       "1            0            0            0            0            0   \n",
       "2        39764        28167         8875         1494        40182   \n",
       "7            0            0        34155        14517        19363   \n",
       "9            0        10818         2776         9421        19118   \n",
       "\n",
       "   encoded_193  encoded_194  helpful  \n",
       "0        14122        23610        1  \n",
       "1            0        30552        0  \n",
       "2         6951        39745        0  \n",
       "7        40032        14122        1  \n",
       "9         3218        14122        1  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # convert to classification problem --> turn helpful to 0s 1s\n",
    "# new_helpful = []\n",
    "# num_positive = 0\n",
    "# for val in df.helpful.tolist():\n",
    "#     if val > 0:\n",
    "#         new_helpful.append(1)\n",
    "#         num_positive += 1\n",
    "#     else:\n",
    "#         new_helpful.append(0)\n",
    "# df.drop(['helpful'], axis=1, inplace=True)\n",
    "# df['helpful'] = new_helpful\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = len(df.columns.tolist())-1\n",
    "VOCAB_SIZE = 41248 # should ideally just transport this from prev\n",
    "EMBED_DIM = 128\n",
    "LSTM_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pytorch model\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "class Attention_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention_Net, self).__init__()\n",
    "        \n",
    "        # define architecture\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "#         self.embedding_dropout = nn.Dropout2d(DROPOUT) # take this out potentially\n",
    "        \n",
    "        self.lstm = nn.LSTM(EMBED_DIM, LSTM_DIM, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # attention layer\n",
    "        self.attention_layer = Attention(LSTM_DIM * 2, MAX_SEQ_LEN)\n",
    "        \n",
    "        self.linear = nn.Linear(LSTM_DIM * 2, 2) # change here to 1 or 2 depending on loss\n",
    "        \n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        #self.softmax = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0)).view(BATCH_SIZE, MAX_SEQ_LEN, -1)\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_lstm_atten = self.attention_layer(h_lstm)\n",
    "        out = self.linear(h_lstm_atten)\n",
    "        softmax_out = self.softmax(out)\n",
    "        return softmax_out\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteamDataset(data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        #'Initialization'\n",
    "        self.data = data\n",
    "        text_cols = [x for x in df.columns.tolist() if x.startswith(\"encoded\")]\n",
    "        self.train = torch.tensor(data[text_cols].values).cuda()\n",
    "        labels = data['recommendation'].tolist()\n",
    "        \n",
    "        self.one_hot_labels = torch.tensor(np.array(labels)).squeeze().type(torch.LongTensor).cuda() # change to longtensor if using custom loss\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        #'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #'Generates one sample of data'\n",
    "        \n",
    "        # Load data and get label\n",
    "        X = self.train[index]\n",
    "        Y = self.one_hot_labels[index]\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = int(0.8 * len(df))\n",
    "steam_dataset = SteamDataset(df[:train_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1ee43183288>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steam_data_loader = data.DataLoader(steam_dataset, batch_size=16, num_workers=0, drop_last=True)\n",
    "steam_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive examples =  32126\n",
      "negative examples =  373543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0860, 12.6274], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_negative = len(df) - num_positive\n",
    "print ('positive examples = ', num_positive)\n",
    "print ('negative examples = ', num_negative)\n",
    "\n",
    "weights = torch.tensor([1/(num_negative / len(df)), 1 / (num_positive / len(df))]).cuda()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = Attention_Net().cuda()\n",
    "# loss_function = nn.CrossEntropyLoss(weight=weights)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(attention_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_binary_cross_entropy(output, target, weights=None):\n",
    "        \n",
    "#     if weights is not None:\n",
    "#         assert len(weights) == 2\n",
    "        \n",
    "#         loss = weights[1] * (target * torch.log(output)) + \\\n",
    "#                weights[0] * ((1 - target) * torch.log(1 - output))\n",
    "#     else:\n",
    "#         loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)\n",
    "\n",
    "#     return torch.neg(torch.mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 10000 train batches | took 143.3994164466858 seconds | loss: 5488.24365234375\n",
      "For 10000 train batches | took 149.5966293811798 seconds | loss: 6264.8916015625\n",
      "On epoch: 2\n",
      "For 10000 train batches | took 156.15515518188477 seconds | loss: 5442.53515625\n",
      "For 10000 train batches | took 157.86287260055542 seconds | loss: 6072.287109375\n",
      "On epoch: 3\n",
      "For 10000 train batches | took 157.92653799057007 seconds | loss: 5225.564453125\n",
      "For 10000 train batches | took 156.9766068458557 seconds | loss: 5818.39111328125\n",
      "On epoch: 4\n",
      "For 10000 train batches | took 156.3397512435913 seconds | loss: 4925.81591796875\n",
      "For 10000 train batches | took 157.02324628829956 seconds | loss: 5508.9091796875\n",
      "On epoch: 5\n",
      "For 10000 train batches | took 157.9313645362854 seconds | loss: 4692.97900390625\n",
      "For 10000 train batches | took 160.92836380004883 seconds | loss: 5071.08837890625\n",
      "On epoch: 6\n",
      "For 10000 train batches | took 157.11392498016357 seconds | loss: 4404.39892578125\n",
      "For 10000 train batches | took 152.75414967536926 seconds | loss: 4768.91162109375\n",
      "On epoch: 7\n",
      "For 10000 train batches | took 151.36080694198608 seconds | loss: 4213.80224609375\n",
      "For 10000 train batches | took 151.24337816238403 seconds | loss: 4637.02001953125\n",
      "On epoch: 8\n",
      "For 10000 train batches | took 150.92158222198486 seconds | loss: 4147.0322265625\n",
      "For 10000 train batches | took 151.2767779827118 seconds | loss: 4562.66943359375\n",
      "On epoch: 9\n",
      "For 10000 train batches | took 150.76661729812622 seconds | loss: 4110.72802734375\n",
      "For 10000 train batches | took 151.0382010936737 seconds | loss: 4511.72216796875\n",
      "On epoch: 10\n",
      "For 10000 train batches | took 151.07989954948425 seconds | loss: 4088.982666015625\n",
      "For 10000 train batches | took 151.69523859024048 seconds | loss: 4478.1142578125\n",
      "On epoch: 11\n",
      "For 10000 train batches | took 151.12996816635132 seconds | loss: 4066.64453125\n",
      "For 10000 train batches | took 153.28163886070251 seconds | loss: 4447.04541015625\n",
      "On epoch: 12\n",
      "For 10000 train batches | took 154.3751256465912 seconds | loss: 4053.329345703125\n",
      "For 10000 train batches | took 155.56711506843567 seconds | loss: 4422.083984375\n",
      "On epoch: 13\n",
      "For 10000 train batches | took 153.16930508613586 seconds | loss: 4056.4228515625\n",
      "For 10000 train batches | took 153.1019630432129 seconds | loss: 4405.56982421875\n",
      "On epoch: 14\n",
      "For 10000 train batches | took 153.0008635520935 seconds | loss: 4106.06689453125\n",
      "For 10000 train batches | took 151.91140818595886 seconds | loss: 4386.3935546875\n",
      "On epoch: 15\n",
      "For 10000 train batches | took 151.69337797164917 seconds | loss: 4013.014404296875\n",
      "For 10000 train batches | took 154.14903354644775 seconds | loss: 4363.12255859375\n",
      "On epoch: 16\n",
      "For 10000 train batches | took 153.85355472564697 seconds | loss: 4004.550537109375\n",
      "For 10000 train batches | took 153.80396342277527 seconds | loss: 4340.29443359375\n",
      "On epoch: 17\n",
      "For 10000 train batches | took 158.82233881950378 seconds | loss: 3991.62890625\n",
      "For 10000 train batches | took 156.72090005874634 seconds | loss: 4323.2607421875\n",
      "On epoch: 18\n",
      "For 10000 train batches | took 155.0462145805359 seconds | loss: 3982.264892578125\n",
      "For 10000 train batches | took 156.00636529922485 seconds | loss: 4306.3505859375\n",
      "On epoch: 19\n",
      "For 10000 train batches | took 152.28022527694702 seconds | loss: 3972.969970703125\n",
      "For 10000 train batches | took 152.799156665802 seconds | loss: 4290.4921875\n",
      "On epoch: 20\n",
      "For 10000 train batches | took 151.60451889038086 seconds | loss: 3963.029296875\n",
      "For 10000 train batches | took 151.96823143959045 seconds | loss: 4279.58447265625\n",
      "Took 6237.355663776398 seconds\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "EPOCHS = 20\n",
    "checkpoint_num = 10000\n",
    "start = time.time()\n",
    "\n",
    "running_loss = 0\n",
    "for i in range(EPOCHS):\n",
    "    print (\"On epoch: {}\".format(i+1))\n",
    "    idx = 0\n",
    "    second_start = time.time()\n",
    "    for train_X, train_Y in steam_data_loader:         \n",
    "        attention_model.zero_grad()\n",
    "        pred_y = attention_model(train_X)       \n",
    "\n",
    "        # loss = weighted_binary_cross_entropy(pred_y, train_Y, weights)\n",
    "\n",
    "        loss = loss_function(pred_y, train_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        idx+=1\n",
    "    print ('Epoch {} | took {} seconds | summed loss: {} | avg loss: {}'\n",
    "                   .format(i+1, time.time() - second_start, running_loss, running_loss / len(steam_dataset)))\n",
    "        \n",
    "print (\"Took {} seconds\".format(time.time() - start))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_eval_dataset = SteamDataset(df[train_num:])\n",
    "steam_eval_data_loader = data.DataLoader(steam_eval_dataset, batch_size=16, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy: 0.8540069514630118\n",
      "Eval loss: 2304.1591796875\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "correct = 0\n",
    "eval_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (test_X, test_Y) in enumerate(steam_eval_data_loader):\n",
    "        preds = attention_model(test_X)\n",
    "        for idx, each_pred in enumerate(preds):\n",
    "            if each_pred[0] >= 0.5 and test_Y[idx] == 0:\n",
    "                correct += 1\n",
    "            elif each_pred[0] < 0.5 and test_Y[idx] == 1:\n",
    "                correct += 1\n",
    "        loss = loss_function(preds, test_Y)\n",
    "        eval_loss += loss\n",
    "        \n",
    "        \n",
    "print (\"Eval accuracy: {}\".format(correct / len(steam_eval_dataset)))\n",
    "print (\"Eval summed loss: {} | avg loss: {}\".format(eval_loss, eval_loss / len(steam_eval_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'models/word200_date3-31_epoch20.pt'\n",
    "torch.save(attention_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
